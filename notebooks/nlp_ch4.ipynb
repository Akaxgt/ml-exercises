{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(input_text):\n",
    "    \n",
    "    def decode_encoding(text):\n",
    "        # Remove start and end tags\n",
    "        decoded_text = re.sub(r\"<.*?>\", \"\", input_text)\n",
    "        # Replace newline characters with spaces\n",
    "        decoded_text = re.sub(r'\\n', ' ', decoded_text)\n",
    "        # Normalize multiple spaces to a single space\n",
    "        decoded_text = re.sub(r'\\s+', ' ', decoded_text).strip()\n",
    "\n",
    "        # Insert period after \"Employees details\" if it directly precedes ' Attached'\n",
    "        return re.sub(r\"Employees details\\s*Attached\", \"Employees details. Attached\", decoded_text)\n",
    "\n",
    "    def lowercase_text(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def digits_to_words(text):\n",
    "        ordinal_dict = {\n",
    "            '1st': 'first',\n",
    "            '2nd': 'second',\n",
    "        }\n",
    "        text = re.sub(r'\\b1st\\b', 'first', text)\n",
    "        text = re.sub(r'\\b2nd\\b', 'second', text)\n",
    "        return re.sub(r'\\d', lambda x: num2words(int(x.group())), text)\n",
    "    \n",
    "    def remove_punctuation(text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def correct_spelling(text):\n",
    "        spelling_corrections = {\n",
    "            \"pairoll\": \"payroll\",\n",
    "            \"healtcare\": \"healthcare\"\n",
    "        }\n",
    "        return ' '.join(spelling_corrections.get(word, word) for word in text.split())\n",
    "    \n",
    "    def remove_stopwords(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    def perform_stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return ' '.join(stemmer.stem(word) for word in text.split())\n",
    "    \n",
    "    def perform_lemmatization(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    \n",
    "    decoded_text = decode_encoding(input_text)\n",
    "    print(\"Step 1: Decoding text:\", decoded_text)\n",
    "\n",
    "    lowercased_text = lowercase_text(decoded_text)\n",
    "    print(\"Step 2: Lowercasing text:\", lowercased_text)\n",
    "\n",
    "    digits_to_words_text = digits_to_words(lowercased_text)\n",
    "    print(\"Step 3: Digits to words:\", digits_to_words_text)\n",
    "\n",
    "    no_punctuation_text = remove_punctuation(digits_to_words_text)\n",
    "    print(\"Step 4: Punctuation and special characters' removal:\", no_punctuation_text)\n",
    "\n",
    "    spelling_corrected_text = correct_spelling(no_punctuation_text)\n",
    "    print(\"Step 5: Spelling corrections:\", spelling_corrected_text)\n",
    "\n",
    "    stopword_removed_text = remove_stopwords(spelling_corrected_text)\n",
    "    print(\"Step 6: Stopword removal:\", stopword_removed_text)\n",
    "\n",
    "    stemmed_text = perform_stemming(stopword_removed_text)\n",
    "    print(\"Step 7: Stemming:\", stemmed_text)\n",
    "\n",
    "    lemmatized_text = perform_lemmatization(stemmed_text)\n",
    "    print(\"Step 8: Lemmatizing:\", lemmatized_text)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Decoding text: Employees details. Attached are 2 files, 1st one is pairoll, 2nd is healtcare!\n",
      "Step 2: Lowercasing text: employees details. attached are 2 files, 1st one is pairoll, 2nd is healtcare!\n",
      "Step 3: Digits to words: employees details. attached are two files, first one is pairoll, second is healtcare!\n",
      "Step 4: Punctuation and special characters' removal: employees details attached are two files first one is pairoll second is healtcare\n",
      "Step 5: Spelling corrections: employees details attached are two files first one is payroll second is healthcare\n",
      "Step 6: Stopword removal: employees details attached two files first one payroll second healthcare\n",
      "Step 7: Stemming: employe detail attach two file first one payrol second healthcar\n",
      "Step 8: Lemmatizing: employe detail attach two file first one payrol second healthcar\n",
      "Final output: employe detail attach two file first one payrol second healthcar\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<SUBJECT LINE> Employees details<END><BODY TEXT>Attached are 2 files,\\n1st one is pairoll, 2nd is healtcare!<END>\"\n",
    "result = text_preprocessing(input_text)\n",
    "print(\"Final output:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER & POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner_and_pos(text):\n",
    "    # Load the spaCy model\n",
    "    # Load the spaCy model globally to avoid reloading it multiple times\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Named Entity Recognition (NER)\n",
    "    ner_output = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Part-of-Speech (POS) tagging\n",
    "    pos_output = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "    return ner_output, pos_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Output:\n",
      "quarterly DATE\n",
      "tomorrow DATE\n",
      "Microsoft ORG\n",
      "4pm TIME\n",
      "Google ORG\n",
      "4pm TIME\n",
      "AT&T ORG\n",
      "6pm TIME\n",
      "\n",
      "POS Output:\n",
      "('The', 'DET')\n",
      "('companies', 'NOUN')\n",
      "('that', 'PRON')\n",
      "('would', 'AUX')\n",
      "('be', 'AUX')\n",
      "('releasing', 'VERB')\n",
      "('their', 'PRON')\n",
      "('quarterly', 'ADJ')\n",
      "('reports', 'NOUN')\n",
      "('tomorrow', 'NOUN')\n",
      "('are', 'AUX')\n",
      "('Microsoft', 'PROPN')\n",
      "(',', 'PUNCT')\n",
      "('4', 'NUM')\n",
      "('pm', 'NOUN')\n",
      "(',', 'PUNCT')\n",
      "('Google', 'PROPN')\n",
      "(',', 'PUNCT')\n",
      "('4', 'NUM')\n",
      "('pm', 'NOUN')\n",
      "(',', 'PUNCT')\n",
      "('and', 'CCONJ')\n",
      "('AT&T', 'PROPN')\n",
      "(',', 'PUNCT')\n",
      "('6', 'NUM')\n",
      "('pm', 'NOUN')\n",
      "('.', 'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The companies that would be releasing their quarterly reports tomorrow are Microsoft, 4pm, Google, 4pm, and AT&T, 6pm.\"\n",
    "ner_result, pos_result = perform_ner_and_pos(input_text)\n",
    "\n",
    "print(\"NER Output:\")\n",
    "for ent in ner_result:\n",
    "    print(f\"{ent[0]} {ent[1]}\")\n",
    "\n",
    "print(\"\\nPOS Output:\")\n",
    "for token_pos in pos_result:\n",
    "    print(token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
